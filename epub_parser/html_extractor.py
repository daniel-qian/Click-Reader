#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EPUB HTMLæå–å™¨
ä»EPUBæ–‡ä»¶ä¸­æå–æ‰€æœ‰HTML/XHTMLå†…å®¹ï¼Œä¿æŒåŸå§‹æ ¼å¼
æ”¯æŒè‡ªåŠ¨è¿‡æ»¤å™ªå£°é¡µé¢å’Œé…ç½®åŒ–è§„åˆ™
"""

import os
import sys
import argparse
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple

try:
    import ebooklib
    from ebooklib import epub
except ImportError:
    print("è¯·å®‰è£…ebooklibåº“: pip install EbookLib")
    exit(1)

from bs4 import BeautifulSoup, Comment

# å¯¼å…¥é…ç½®æ–‡ä»¶
from config import (
    NOISE_TITLES, NOISE_FILENAMES, NOISE_CSS_SELECTORS, NOISE_HTML_TAGS,
    MEANINGFUL_TAGS, COVER_INDICATORS, MIN_TEXT_LENGTH, MIN_MEANINGFUL_TAGS,
    DEFAULT_CONFIG
)

class EPUBHTMLExtractor:
    def __init__(self, epub_path: str, output_dir: str = "extracted_html", config: Dict[str, Any] = None):
        self.epub_path = Path(epub_path)
        self.output_dir = Path(output_dir)
        self.book = None
        self.book_name = self.epub_path.stem
        self.metadata = {}
        self.spine_info = []
        self.toc_info = []
        self.skipped_files = []  # è®°å½•è¢«è·³è¿‡çš„æ–‡ä»¶
        
        # åˆå¹¶é…ç½®
        self.config = DEFAULT_CONFIG.copy()
        if config:
            self.config.update(config)
            
        # è®¾ç½®æ—¥å¿—
        log_level = logging.DEBUG if self.config.get('verbose', False) else logging.INFO
        logging.basicConfig(level=log_level, format='%(levelname)s: %(message)s')
        self.logger = logging.getLogger(__name__)
        
    def load_epub(self) -> bool:
        """åŠ è½½EPUBæ–‡ä»¶"""
        try:
            print(f"ğŸ“š æ­£åœ¨åŠ è½½EPUBæ–‡ä»¶: {self.epub_path}")
            self.book = epub.read_epub(str(self.epub_path))
            print(f"âœ… EPUBæ–‡ä»¶åŠ è½½æˆåŠŸ")
            return True
        except Exception as e:
            print(f"âŒ EPUBæ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            return False
    
    def extract_metadata(self):
        """æå–ä¹¦ç±å…ƒæ•°æ®"""
        self.metadata = {
            'title': 'Unknown',
            'author': 'Unknown',
            'language': 'zh'
        }
        
        if self.book:
            # è·å–æ ‡é¢˜
            title = self.book.get_metadata('DC', 'title')
            if title:
                self.metadata['title'] = title[0][0]
            
            # è·å–ä½œè€…
            creator = self.book.get_metadata('DC', 'creator')
            if creator:
                self.metadata['author'] = creator[0][0]
            
            # è·å–è¯­è¨€
            language = self.book.get_metadata('DC', 'language')
            if language:
                self.metadata['language'] = language[0][0]
        
        print(f"ä¹¦ç±ä¿¡æ¯:")
        print(f"  æ ‡é¢˜: {self.metadata['title']}")
        print(f"  ä½œè€…: {self.metadata['author']}")
        print(f"  è¯­è¨€: {self.metadata['language']}")
    
    def is_noise_page(self, item, content: str = None) -> Tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå™ªå£°é¡µé¢ï¼Œè¿”å› (æ˜¯å¦ä¸ºå™ªå£°, è·³è¿‡åŸå› )"""
        if not self.config.get('skip_noise_pages', True):
            return False, ""
            
        # æ£€æŸ¥æ–‡ä»¶å
        file_name = item.file_name.lower()
        for noise_keyword in NOISE_FILENAMES:
            if noise_keyword.lower() in file_name:
                return True, f"æ–‡ä»¶ååŒ…å«å™ªå£°å…³é”®å­—: {noise_keyword}"
        
        # å¦‚æœæœ‰å†…å®¹ï¼Œè¿›ä¸€æ­¥æ£€æŸ¥
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            
            # æ£€æŸ¥æ ‡é¢˜
            title_tags = soup.find_all(['h1', 'h2', 'h3', 'title'])
            for tag in title_tags:
                title_text = tag.get_text(strip=True)
                for noise_title in NOISE_TITLES:
                    if noise_title in title_text:
                        return True, f"æ ‡é¢˜åŒ…å«å™ªå£°å…³é”®å­—: {noise_title}"
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç©ºç™½é¡µ
            text_content = soup.get_text(strip=True)
            meaningful_tags = soup.find_all(lambda tag: tag.name in MEANINGFUL_TAGS)
            
            if (len(text_content) < self.config.get('min_text_length', MIN_TEXT_LENGTH) and 
                len(meaningful_tags) < MIN_MEANINGFUL_TAGS):
                # æ£€æŸ¥æ˜¯å¦ä¸ºå°é¢é¡µ
                if self.config.get('keep_cover', True) and self._is_cover_page(soup, file_name):
                    return False, ""
                return True, f"ç©ºç™½é¡µé¢ (æ–‡æœ¬é•¿åº¦: {len(text_content)}, æœ‰æ„ä¹‰æ ‡ç­¾: {len(meaningful_tags)})"
        
        return False, ""
    
    def _is_cover_page(self, soup: BeautifulSoup, file_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå°é¢é¡µ"""
        # æ£€æŸ¥æ–‡ä»¶å
        for cover_keyword in COVER_INDICATORS['filenames']:
            if cover_keyword.lower() in file_name.lower():
                return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å°é¢ç›¸å…³çš„å›¾ç‰‡æˆ–å…ƒç´ 
        for tag_name in COVER_INDICATORS['tags']:
            for tag in soup.find_all(tag_name):
                # æ£€æŸ¥classå±æ€§
                tag_classes = tag.get('class', [])
                for cover_class in COVER_INDICATORS['classes']:
                    if cover_class in tag_classes:
                        return True
                
                # æ£€æŸ¥idå±æ€§
                tag_id = tag.get('id', '')
                for cover_id in COVER_INDICATORS['ids']:
                    if cover_id in tag_id:
                        return True
        
        return False
    
    def extract_spine_info(self):
        """æå–spineä¿¡æ¯ï¼ˆé˜…è¯»é¡ºåºï¼‰"""
        self.spine_info = []
        
        print("\næå–spineä¿¡æ¯:")
        for idx, (item_id, linear) in enumerate(self.book.spine):
            item = self.book.get_item_with_id(item_id)
            if item and item.get_type() == ebooklib.ITEM_DOCUMENT:
                spine_item = {
                    'index': idx,
                    'item_id': item_id,
                    'file_name': item.file_name,
                    'linear': linear,
                    'media_type': item.media_type
                }
                self.spine_info.append(spine_item)
                print(f"  [{idx:02d}] {item.file_name} (linear: {linear})")
        
        print(f"\nå…±æ‰¾åˆ° {len(self.spine_info)} ä¸ªæ–‡æ¡£é¡¹ç›®")
    
    def extract_toc_info(self):
        """æå–ç›®å½•ä¿¡æ¯"""
        self.toc_info = []
        
        def process_toc_item(item, level=0):
            if hasattr(item, 'title') and hasattr(item, 'href'):
                self.toc_info.append({
                    'title': item.title,
                    'href': item.href,
                    'level': level
                })
            
            # å¤„ç†å­é¡¹ç›®
            if hasattr(item, '__iter__') and not isinstance(item, str):
                for sub_item in item:
                    if hasattr(sub_item, 'title'):
                        process_toc_item(sub_item, level + 1)
        
        print("\næå–ç›®å½•ä¿¡æ¯:")
        for item in self.book.toc:
            process_toc_item(item)
        
        print(f"å…±æ‰¾åˆ° {len(self.toc_info)} ä¸ªç›®å½•é¡¹")
    
    def clean_html_content(self, content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹ï¼Œç§»é™¤ä¸å¿…è¦çš„å…ƒç´ ä½†ä¿æŒç»“æ„"""
        soup = BeautifulSoup(content, 'html.parser')
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦ç§»é™¤æ³¨é‡Š
        if not self.config.get('preserve_comments', True):
            comments = soup.find_all(string=lambda text: isinstance(text, Comment))
            for comment in comments:
                comment.extract()
        
        # ç§»é™¤å™ªå£°HTMLæ ‡ç­¾
        for tag_name in NOISE_HTML_TAGS:
            for tag in soup.find_all(tag_name):
                self.logger.debug(f"ç§»é™¤å™ªå£°æ ‡ç­¾: {tag_name}")
                tag.decompose()
        
        # ç§»é™¤å™ªå£°CSSé€‰æ‹©å™¨åŒ¹é…çš„å…ƒç´ 
        for selector in NOISE_CSS_SELECTORS:
            try:
                for element in soup.select(selector):
                    self.logger.debug(f"ç§»é™¤å™ªå£°å…ƒç´ : {selector}")
                    element.decompose()
            except Exception as e:
                self.logger.warning(f"CSSé€‰æ‹©å™¨ {selector} è§£æå¤±è´¥: {e}")
        
        # ç§»é™¤åŒ…å«å™ªå£°å…³é”®è¯çš„æ®µè½
        for p in soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            text = p.get_text(strip=True)
            if text:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å™ªå£°æ ‡é¢˜å…³é”®è¯
                for noise_title in NOISE_TITLES:
                    if noise_title.lower() in text.lower():
                        self.logger.debug(f"ç§»é™¤å™ªå£°å†…å®¹æ®µè½: {text[:50]}...")
                        p.decompose()
                        break
                else:
                    # æ£€æŸ¥ç‰ˆæƒä¿¡æ¯
                    if any(keyword in text.lower() for keyword in ['copyright', 'ç‰ˆæƒ', 'isbn', 'penguin', 'viking', 'imprint']):
                        self.logger.debug(f"ç§»é™¤ç‰ˆæƒä¿¡æ¯æ®µè½: {text[:50]}...")
                        p.decompose()
                    # æ£€æŸ¥ç›®å½•é“¾æ¥
                    elif text.lower().strip() in ['contents', 'ç›®å½•', 'table of contents'] or \
                         (p.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6'] and 'contents' in text.lower()):
                        self.logger.debug(f"ç§»é™¤ç›®å½•æ ‡é¢˜: {text}")
                        p.decompose()
        
        # ç§»é™¤ç›®å½•é“¾æ¥æ®µè½ï¼ˆæ›´ç²¾ç¡®çš„è¯†åˆ«ï¼‰
        # åªåœ¨æ•´ä¸ªé¡µé¢åŒ…å«å¤§é‡é“¾æ¥æ—¶æ‰è€ƒè™‘åˆ é™¤ç›®å½•
        all_links = soup.find_all('a')
        if len(all_links) > 10:  # åªæœ‰å½“é¡µé¢åŒ…å«å¾ˆå¤šé“¾æ¥æ—¶æ‰è¿›è¡Œç›®å½•æ¸…ç†
            for p in soup.find_all('p'):
                links = p.find_all('a')
                if len(links) >= 1:  # æ£€æŸ¥å•ä¸ªé“¾æ¥æ®µè½
                    link_texts = [a.get_text(strip=True) for a in links]
                    # æ›´ä¸¥æ ¼çš„ç›®å½•è¯†åˆ«ï¼šå¿…é¡»åŒ…å«æ˜ç¡®çš„ç›®å½•å…³é”®è¯
                    if any(any(keyword in link_text.lower() for keyword in ['table of contents', 'contents', 'ç›®å½•', 'index']) 
                           for link_text in link_texts):
                        self.logger.debug(f"ç§»é™¤ç›®å½•é“¾æ¥æ®µè½: {p.get_text(strip=True)[:50]}...")
                        p.decompose()
        
        # ç§»é™¤ç©ºçš„æ®µè½å’Œdivï¼ˆä½†ä¿ç•™æœ‰å›¾ç‰‡çš„ï¼‰
        for tag in soup.find_all(['p', 'div']):
            if not tag.get_text(strip=True) and not tag.find('img'):
                tag.decompose()
        
        return str(soup)
    
    def extract_all_html_files(self):
        """æå–æ‰€æœ‰HTMLæ–‡ä»¶ï¼Œç”Ÿæˆæœªæ¸…ç†å’Œæ¸…ç†ç‰ˆæœ¬"""
        if not self.spine_info:
            print("é”™è¯¯ï¼šæœªæ‰¾åˆ°spineä¿¡æ¯")
            return False
        
        # ä½¿ç”¨EPUBæ–‡ä»¶åä½œä¸ºè¾“å‡ºç›®å½•
        epub_name = self.epub_path.stem
        safe_name = "".join(c for c in epub_name if c.isalnum() or c in (' ', '-', '_')).strip()
        base_output_path = self.output_dir / safe_name
        
        # åˆ›å»ºæœªæ¸…ç†å’Œæ¸…ç†ç‰ˆæœ¬çš„è¾“å‡ºç›®å½•
        raw_output_path = base_output_path / "raw_html"
        cleaned_output_path = base_output_path / "cleaned_html"
        raw_output_path.mkdir(parents=True, exist_ok=True)
        cleaned_output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\nå¼€å§‹æå–HTMLæ–‡ä»¶:")
        print(f"  - æœªæ¸…ç†ç‰ˆæœ¬: {raw_output_path}")
        print(f"  - æ¸…ç†ç‰ˆæœ¬: {cleaned_output_path}")
        if self.config.get('skip_noise_pages', True):
            print("  - å¯ç”¨å™ªå£°é¡µé¢è¿‡æ»¤")
        
        extracted_files = []
        
        for spine_item in self.spine_info:
            item = self.book.get_item_with_id(spine_item['item_id'])
            if not item:
                continue
            
            try:
                # è·å–åŸå§‹å†…å®¹
                content = item.get_content().decode('utf-8')
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºå™ªå£°é¡µé¢
                is_noise, skip_reason = self.is_noise_page(item, content)
                if is_noise:
                    self.skipped_files.append({
                        'index': spine_item['index'],
                        'file_name': spine_item['file_name'],
                        'item_id': spine_item['item_id'],
                        'skip_reason': skip_reason
                    })
                    self.logger.debug(f"è·³è¿‡å™ªå£°é¡µé¢: {spine_item['file_name']} - {skip_reason}")
                    print(f"  âŠ˜ è·³è¿‡: {spine_item['file_name']} ({skip_reason})")
                    continue
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
                original_name = spine_item['file_name']
                base_name = Path(original_name).stem
                output_filename = f"chapter_{spine_item['index']:03d}_{base_name}.html"
                
                # ä¿å­˜æœªæ¸…ç†ç‰ˆæœ¬
                raw_file_path = raw_output_path / output_filename
                with open(raw_file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                # æ¸…ç†å†…å®¹å¹¶ä¿å­˜æ¸…ç†ç‰ˆæœ¬
                cleaned_content = self.clean_html_content(content)
                cleaned_file_path = cleaned_output_path / output_filename
                with open(cleaned_file_path, 'w', encoding='utf-8') as f:
                    f.write(cleaned_content)
                
                extracted_files.append({
                    'index': spine_item['index'],
                    'original_name': original_name,
                    'output_name': output_filename,
                    'raw_file_path': str(raw_file_path),
                    'cleaned_file_path': str(cleaned_file_path),
                    'raw_size_bytes': len(content.encode('utf-8')),
                    'cleaned_size_bytes': len(cleaned_content.encode('utf-8'))
                })
                
                print(f"  âœ“ æå–: {original_name} -> {output_filename}")
                
            except Exception as e:
                print(f"  âœ— æå–å¤±è´¥: {spine_item['file_name']} - {e}")
                self.logger.error(f"æå–å¤±è´¥: {spine_item['file_name']} - {e}")
        
        # ç”Ÿæˆæå–æŠ¥å‘Š
        report = {
            'metadata': self.metadata,
            'extraction_summary': {
                'total_files_extracted': len(extracted_files),
                'total_files_skipped': len(self.skipped_files),
                'raw_output_directory': str(raw_output_path),
                'cleaned_output_directory': str(cleaned_output_path),
                'epub_source': str(self.epub_path),
                'config_used': self.config
            },
            'spine_info': self.spine_info,
            'extracted_files': extracted_files,
            'skipped_files': self.skipped_files
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = base_output_path / 'extraction_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"\næå–å®Œæˆï¼")
        print(f"  - æˆåŠŸæå–: {len(extracted_files)} ä¸ªæ–‡ä»¶")
        print(f"  - è·³è¿‡æ–‡ä»¶: {len(self.skipped_files)} ä¸ª")
        print(f"  - æœªæ¸…ç†ç‰ˆæœ¬: {raw_output_path}")
        print(f"  - æ¸…ç†ç‰ˆæœ¬: {cleaned_output_path}")
        print(f"  - æå–æŠ¥å‘Š: {report_path}")
        
        return True
    
    def preview_first_chapter(self) -> str:
        """é¢„è§ˆç¬¬ä¸€ç« å†…å®¹"""
        if not self.book:
            return "è¯·å…ˆåŠ è½½EPUBæ–‡ä»¶"
        
        if not self.spine_info:
            return "æœªæ‰¾åˆ°å¯è¯»å–çš„ç« èŠ‚"
        
        # è·å–ç¬¬ä¸€ä¸ªçº¿æ€§ç« èŠ‚
        first_chapter = None
        for item in self.spine_info:
            if item['linear'] == 'yes':
                first_chapter = item
                break
        
        if not first_chapter:
            first_chapter = self.spine_info[0]
        
        item = self.book.get_item_with_id(first_chapter['item_id'])
        if item:
            html_content = item.get_content().decode('utf-8')
            cleaned_html = self.clean_html_content(html_content)
            
            # åªæ˜¾ç¤ºå‰1000ä¸ªå­—ç¬¦ä½œä¸ºé¢„è§ˆ
            preview = cleaned_html[:1000] + "..." if len(cleaned_html) > 1000 else cleaned_html
            return f"ç¬¬ä¸€ç« é¢„è§ˆ ({first_chapter['file_name']}):\n\n{preview}"
        
        return "æ— æ³•è¯»å–ç¬¬ä¸€ç« å†…å®¹"

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='EPUB HTMLæå–å™¨ - ä»EPUBæ–‡ä»¶ä¸­æå–HTMLå†…å®¹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python html_extractor.py                          # äº¤äº’å¼é€‰æ‹©EPUBæ–‡ä»¶
  python html_extractor.py book.epub                # å¤„ç†æŒ‡å®šæ–‡ä»¶
  python html_extractor.py --no-skip-noise          # ä¸è·³è¿‡å™ªå£°é¡µé¢
  python html_extractor.py --no-preserve-comments   # ä¸ä¿ç•™HTMLæ³¨é‡Š
  python html_extractor.py --verbose                # æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
        """
    )
    
    parser.add_argument(
        'epub_file', 
        nargs='?', 
        help='è¦å¤„ç†çš„EPUBæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™äº¤äº’å¼é€‰æ‹©ï¼‰'
    )
    
    parser.add_argument(
        '--output-dir', '-o',
        default='extracted_html',
        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: extracted_htmlï¼‰'
    )
    
    parser.add_argument(
        '--preserve-comments',
        action='store_true',
        default=True,
        help='ä¿ç•™HTMLæ³¨é‡Šï¼ˆé»˜è®¤å¯ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--no-preserve-comments',
        action='store_false',
        dest='preserve_comments',
        help='ä¸ä¿ç•™HTMLæ³¨é‡Š'
    )
    
    parser.add_argument(
        '--skip-noise-pages',
        action='store_true',
        default=True,
        help='è·³è¿‡å™ªå£°é¡µé¢ï¼ˆé»˜è®¤å¯ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--no-skip-noise',
        action='store_false',
        dest='skip_noise_pages',
        help='ä¸è·³è¿‡å™ªå£°é¡µé¢'
    )
    
    parser.add_argument(
        '--keep-cover',
        action='store_true',
        default=True,
        help='ä¿ç•™å°é¢é¡µï¼ˆé»˜è®¤å¯ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--no-keep-cover',
        action='store_false',
        dest='keep_cover',
        help='ä¸ä¿ç•™å°é¢é¡µ'
    )
    
    parser.add_argument(
        '--min-text-length',
        type=int,
        default=MIN_TEXT_LENGTH,
        help=f'æœ€å°æ–‡æœ¬é•¿åº¦é˜ˆå€¼ï¼ˆé»˜è®¤: {MIN_TEXT_LENGTH}ï¼‰'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )
    
    return parser.parse_args()

def select_epub_files(epub_file_arg: Optional[str] = None) -> List[Path]:
    """é€‰æ‹©EPUBæ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰"""
    if epub_file_arg:
        epub_path = Path(epub_file_arg)
        if epub_path.exists() and epub_path.suffix.lower() == '.epub':
            return [epub_path]
        else:
            print(f"é”™è¯¯: æ–‡ä»¶ {epub_file_arg} ä¸å­˜åœ¨æˆ–ä¸æ˜¯EPUBæ–‡ä»¶")
            return []
    
    # æŸ¥æ‰¾å½“å‰ç›®å½•å’Œepub-fileså­ç›®å½•ä¸‹çš„EPUBæ–‡ä»¶
    current_dir = Path.cwd()
    epub_files = list(current_dir.glob("*.epub"))
    epub_files.extend(list(current_dir.glob("epub-files/*.epub")))
    
    if not epub_files:
        print("å½“å‰ç›®å½•ä¸‹æœªæ‰¾åˆ°EPUBæ–‡ä»¶")
        return []
    
    if len(epub_files) == 1:
        epub_path = epub_files[0]
        print(f"æ‰¾åˆ°EPUBæ–‡ä»¶: {epub_path}")
        return [epub_path]
    else:
        print("æ‰¾åˆ°å¤šä¸ªEPUBæ–‡ä»¶:")
        for i, epub_file in enumerate(epub_files, 1):
            print(f"  {i}. {epub_file.name}")
        
        while True:
            try:
                choice_input = input("\nè¯·é€‰æ‹©è¦å¤„ç†çš„æ–‡ä»¶ç¼–å·ï¼ˆå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 1,3,5ï¼‰: ")
                if not choice_input.strip():
                    print("è¯·è¾“å…¥æœ‰æ•ˆçš„ç¼–å·")
                    continue
                
                choices = [int(x.strip()) - 1 for x in choice_input.split(',')]
                selected_files = []
                
                for choice in choices:
                    if 0 <= choice < len(epub_files):
                        selected_files.append(epub_files[choice])
                    else:
                        print(f"ç¼–å· {choice + 1} æ— æ•ˆï¼Œå·²è·³è¿‡")
                
                if selected_files:
                    return selected_files
                else:
                    print("æœªé€‰æ‹©æœ‰æ•ˆæ–‡ä»¶")
                    
            except ValueError:
                print("è¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
            except EOFError:
                print("\nç”¨æˆ·å–æ¶ˆæ“ä½œ")
                return []

def main():
    args = parse_arguments()
    
    # æ„å»ºé…ç½®
    config = {
        'preserve_comments': args.preserve_comments,
        'skip_noise_pages': args.skip_noise_pages,
        'keep_cover': args.keep_cover,
        'min_text_length': args.min_text_length,
        'verbose': args.verbose,
    }
    
    # é€‰æ‹©EPUBæ–‡ä»¶ï¼ˆæ”¯æŒå¤šé€‰ï¼‰
    epub_paths = select_epub_files(args.epub_file)
    if not epub_paths:
        return
    
    print(f"\nå‡†å¤‡å¤„ç† {len(epub_paths)} ä¸ªEPUBæ–‡ä»¶")
    
    # å¤„ç†æ¯ä¸ªEPUBæ–‡ä»¶
    for i, epub_path in enumerate(epub_paths, 1):
        print(f"\n{'='*60}")
        print(f"å¤„ç†ç¬¬ {i}/{len(epub_paths)} ä¸ªæ–‡ä»¶: {epub_path.name}")
        print(f"{'='*60}")
        
        # åˆ›å»ºæå–å™¨å¹¶æ‰§è¡Œæå–
        extractor = EPUBHTMLExtractor(str(epub_path), args.output_dir, config)
        
        if extractor.load_epub():
            extractor.extract_metadata()
            extractor.extract_toc_info()
            extractor.extract_spine_info()
            extractor.extract_all_html_files()
        else:
            print(f"EPUBæ–‡ä»¶åŠ è½½å¤±è´¥: {epub_path}")
    
    print(f"\n{'='*60}")
    print(f"æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼å…±å¤„ç†äº† {len(epub_paths)} ä¸ªEPUBæ–‡ä»¶")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()